{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on tourists to the United States\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The project provides data in a data warehouse located on Redshift to report on and analyze tourist/immigrant travel behavior and patterns in relation to a state's characteristics. The scope is limited to tourists/immigrants entering the United States.\n",
    "\n",
    "The project combines data from US immigration authories detailing when, why and how people came to the United States as well as US demographics and airport codes.\n",
    "\n",
    "The setup is aimed at giving a lot of flexibility in using the data while ensuring high quality of the data. This way different target groups address a variety of questions and can get value from the data. However, the focus is on providing reporting capabilities to the US authorities. US authorities can e.g. report on the number of tourists/immigrants who came to the US and relate visitors' travel patterns to a state's characteristics.\n",
    "\n",
    "Since the target groups and their goals are quite diverse, it is important to keep the skill threshold necessary to interact with the data low. For this reason, descriptions on all codes, e.g. travel or airport codes are provided. It saves the user time when creating a report. Additionally, Redshift is used since fewer skills are needed than when using e.g. Apache Spark. \n",
    "\n",
    "Since the data is saved in S3 buckets, it allows for the data to be easily scaled if the project meets the approval of the users. More on how the project can be scaled will be detailed under step 5.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, DateType as Date, TimestampType as Timestamp\n",
    "from datetime import datetime, timedelta\n",
    "import pyspark.sql.functions\n",
    "from pyspark.sql.functions import udf, col, expr\n",
    "import os\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "The plan is to provide high quality data on visitors to the United States in a data warehouse located on Redshift to allow for easy reports and analyses. Since visitors generate GDP and influence infrastructure decisions, it is important to base decisions on high quality data.\n",
    "\n",
    "The project combines data from US immigration authories detailing when, why and how people came to the United States as well as US demographic data on population composition. Lastly, airport codes are provided for easier analysis.\n",
    "\n",
    "The aim is to give different target groups the opportunity to get value from the data. However, the focus is on providing reporting capabilities to the US authorities allowing them to drill down or slice and dice the data as desired. US authorities can report on the number of visitors who came to the US and give details on their distribution to US states, countries of origin and visa type. They also can find out if visitors prefer to travel to rural or urban areas when relating immigration to demographic data provided in this project. This can be valuable when it comes to touristic development of areas in the US. Furthermore, data scientists can analyze travel patterns based on country of origin and age to forecast the number of tourists/immigrants per state or city. \n",
    "\n",
    "Further possible questions that can be answered with the project dataset:\n",
    "1. Do visitors' median age and the median age of visited state correlate?\n",
    "2. Are there relations between number of visitors and their characteristics and state population?\n",
    "3. Do cities or states with a large number of foreign-bors attract more foreign visitors?\n",
    "4. Do visitors on a business visa travel to urban or rural areas?\n",
    "5. How many airports does a state have and how many visitors does it attract?\n",
    "\n",
    "The scope is limited to tourists/immigrants entering the United States. This project sample is only focused on immigration data from April 2016, which has more than three million data points. The project rubric states that the dataset should have more than one million rows of data. It is hence assumed that users report on visitors to the US on a monthly basis. US demographics data is also of the year 2016.\n",
    "\n",
    "Data is gathered and cleaned with the help of Apache Spark since the dataset the project works with is middle to large-sized and Apache Spark is a fast engine for large-scale data processing. Data is then loaded into S3 buckets since it consitutes low-cost storage and provides the possibility to be loaded into a data lake if need be. In a last step the data is loaded into Redshift since it is optimized for OLAP workloads. The goal is to create a data model that allows the users to easily understand and use it.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "•\tVisitor arrivals program (I94 Immigration Data): This data comes from the US National Tourism and Trade Office. Each report contains international visitor arrival statistics by world regions and select countries (including top 20), type of visa, mode of transportation, age groups, states visited (first intended address only), and the top ports of entry (for select countries). Souce: https://travel.trade.gov/research/reports/i94/historical/2016.html\n",
    "•\tU.S. City Demographic Data: This data comes from OpenSoft but originally it is from the US Census Bureau's 2015 American Community Survey. It contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. Source: https://public.opendatasoft.com/explore/dataset/us-cities-demographics/information/\n",
    "•\tAirport Code Table: This is a simple table of airport codes and corresponding cities. Source: https://datahub.io/core/airport-codes#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the config object and read cfg file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "#Accessing the AWS user IAM credentials in the dwh.cfg file using config object\n",
    "os.environ['KEY']=config['AWS']['KEY']\n",
    "os.environ['SECRET']=config['AWS']['SECRET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.2\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigration = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration.show(5)\n",
    "df_immigration.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load airport data\n",
    "df_airport = spark.read.format(\"csv\").option(\"header\", True).load(\"airport-codes_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport.show(5)\n",
    "df_airport.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create song data schema to ensure that schema is inferred correctly\n",
    "demoSchema = R([\n",
    "    Fld(\"city\",Str()),\n",
    "    Fld(\"state_name\",Str()),\n",
    "    Fld(\"median_age\",Dbl()),\n",
    "    Fld(\"male_population\",Dbl()),\n",
    "    Fld(\"female_population\",Dbl()),\n",
    "    Fld(\"total_population\",Dbl()),\n",
    "    Fld(\"number of veterans\",Dbl()),\n",
    "    Fld(\"foreign_born\",Dbl()),\n",
    "    Fld(\"avg_household_size\",Dbl()),\n",
    "    Fld(\"state_code\",Str()),\n",
    "    Fld(\"race\",Str()),\n",
    "    Fld(\"count\",Dbl()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load demographics data\n",
    "df_us_demograhics = spark.read.format(\"csv\").option(\"header\", True).option(\"delimiter\", \";\").schema(demoSchema).load(\"us-cities-demographics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_demograhics.show(5)\n",
    "df_us_demograhics.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "The goal is to clean the data ensuring high quality and eliminating columns with little value while keeping granularity of the data to allow a variety of questions to be answered with it. The next paragraphs give an overview of identified quality issues.\n",
    "\n",
    "#### I94 Immigration Data:\n",
    "The data type is sas7bdat. The dataset contains columns that are deprecated according to the data dictionary I94_SAS_Labels_Description.SAS. It also contains columns with codes but missing code descriptions. Futhermore, there are columns that contain largely null values. Since end users probably won't understand travel, visa, city or country codes, the descriptions in I94_SAS_Labels_Description.SAS need to be provided for the users. So instead of 209 as country of residence, the user can now see that the visitor's residence it Japan. \n",
    "\n",
    "#### US city demographics dataset:\n",
    "The data type is csv. Every city is listed multiple times in the dataset since every line states the number of people per race in the city. Since the only column that contains the value per race is 'count' and every other column constitutes the total across all races, duplicates on cities ocurr. This makes it difficult to join demographics data to other datasets. Hence, I decided to join based on state coded but kept the information on city level to be able to determine a state's composition. For example, does it have a lot of large cities or mostly small ones. Cities in this dataset do not fully align with cities in the other datasets. However, it is not a fallacy since not every city has a port or an airport but it needs to be taken into consideration for the data model.\n",
    "\n",
    "#### Airport dataset:\n",
    "The data type is csv. There is no information on when the dataset was compiled. It is assumed that it lists airports in the world as of the year 2016. The dataset contains all types of airports all over the world. Hence, the dataset needs to be filtered to fit the scope of the project.\n",
    " \n",
    "#### Cleaning Steps\n",
    "\n",
    "#### I94 Immigration Data:\n",
    "SAS date was converted to a standard format since this makes it easier for a user to work with the data.\n",
    "Duplicate records were dropped to ensure quality.\n",
    "Depricated columns or columns containing mostly nulls were dropped since they yield nor informational value.\n",
    "Columns were renamed to make it easier for the user to work with them.\n",
    "Rows containing null values for column 'cicid' were dropped since it is the primary key of the dataset.\n",
    "Data types of columns were cast of necessary to ensure that only the necessary storage space is used.\n",
    "Descriptions on codes were provided as state above.\n",
    "\n",
    "#### US city demographics dataset:\n",
    "The race column 'count' was dropped due to the join problem stated above and race being a social construct. \n",
    "Furthermore, the column 'Number of Veterans' was dropped since it does not relate to travel.\n",
    "Since city is the primary key of this dataset it was ensured, that it does not contain null values\n",
    "\n",
    "#### Airport dataset:\n",
    "Columns on codes that do not relate to other datasets like 'local_code' or 'ident' were dropped since they cannot be used as join criteria and yield no other informational value. GPS data and elevation were also dropped since they do not pertain to visitors' data.\n",
    "Only airport with an IATA code were kept since this is the join criterion for the relation to visitors data.\n",
    "Since the project focuses on visitors to the United States, only US airports are considered. The 'continent' column was consequently dropped since there is only one continent left, namely North America.\n",
    "The dataset contains closed airports and helipads, baloonports and seaplane bases. These are not relevant since visitors will only come to the US via proper airports that are still open. Hence, they were filtered out and only.\n",
    "Duplicates were dropped to ensure quality.\n",
    "The column 'iso_region' was split into country and state code to be able to link it to table us_states.\n",
    "The column 'coordinates' was split into longitude and latitude to save the user the work to split it.\n",
    "Columns were cast to the desired data type if schema was inferred incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Clean airport data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create songs table\n",
    "df_airport = df_airport.select('iata_code', 'name', 'iso_country','iso_region','municipality','coordinates', 'type')\n",
    "df_airport.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter blank iata_codes out since this column will be a primary key and drop duplicates\n",
    "df_airport = df_airport.filter(df_airport.iata_code != '').dropDuplicates()\n",
    "# filter to onyl US airports\n",
    "df_airport = df_airport.filter(df_airport.iso_country == 'US').dropDuplicates()\n",
    "#only airports are relevant, not helipads etc.\n",
    "df_airport = df_airport.filter(df_airport.type.contains('airport'))\n",
    "df_airport.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split iso_region in country and state\n",
    "split_col = pyspark.sql.functions.split(df_airport['iso_region'], '-')\n",
    "df_airport = df_airport.withColumn('country_code', split_col.getItem(0))\n",
    "df_airport = df_airport.withColumn('state_code', split_col.getItem(1))\n",
    "df_airport.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split coordinates into longitude and latitude\n",
    "split_col = pyspark.sql.functions.split(df_airport['coordinates'], ', ')\n",
    "df_airport = df_airport.withColumn('latitude', split_col.getItem(0))\n",
    "df_airport = df_airport.withColumn('longitude', split_col.getItem(1))\n",
    "df_airport.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport = df_airport.drop('coordinates')\n",
    "df_airport = df_airport.drop('country_code')\n",
    "df_airport.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport.printSchema()\n",
    "#cast string coordinates to double\n",
    "df_airport = df_airport.withColumn(\"latitude\", df_airport[\"latitude\"].cast(Dbl()))\n",
    "df_airport = df_airport.withColumn(\"longitude\", df_airport[\"longitude\"].cast(Dbl()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean demographics\n",
    "# select relevant columns\n",
    "df_us_demograhics = df_us_demograhics.select('city', 'state_name', 'median_age','male_population','female_population','total_population', 'foreign_born', 'avg_household_size', 'state_code').dropDuplicates()\n",
    "df_us_demograhics.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that city is not null since it is the primary key\n",
    "df_us_demograhics = df_us_demograhics.filter(df_us_demograhics.city != '')\n",
    "df_us_demograhics.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_demograhics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean immigration data\n",
    "df_immigration.show(5)\n",
    "df_immigration.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sas date to date\n",
    "def convert_datetime(x):\n",
    "    try:\n",
    "        start = datetime(1960, 1, 1)\n",
    "        return start + timedelta(days=int(x))\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert arrival and departure date to date\n",
    "udf_datetime_from_sas = udf(lambda x: convert_datetime(x), T.DateType())\n",
    "df_immigration = df_immigration.withColumn(\"arrival_date\", udf_datetime_from_sas(\"arrdate\")).withColumn(\"departure_date\", udf_datetime_from_sas(\"depdate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that cicid is not null since it is the primary key\n",
    "df_immigration = df_immigration.dropDuplicates()\n",
    "df_immigration.printSchema(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only relevant columns\n",
    "df_immigration = df_immigration.select('cicid', col(\"i94yr\").alias(\"year\"), \n",
    "                                       col(\"i94mon\").alias(\"month\"),\n",
    "                                       col(\"i94cit\").alias(\"city_code_origin\"),\n",
    "                                       col(\"i94res\").alias(\"country_code_residence\"),\n",
    "                                       col(\"i94port\").alias(\"city_code_destination\"),\n",
    "                                       col(\"arrival_date\"),\n",
    "                                       col(\"i94mode\").alias(\"travel_code\"),\n",
    "                                       col(\"i94addr\").alias(\"state_code_residence\"),\n",
    "                                       col(\"departure_date\"),\n",
    "                                       col(\"i94visa\").alias(\"visa_code\"),\n",
    "                                       col(\"biryear\").alias(\"birth_year\"),\n",
    "                                       col(\"gender\"),\n",
    "                                       col(\"airline\")\n",
    "                                      ).distinct()\n",
    "df_immigration.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete nulls in cicid since it will be the primary key. No null values expected\n",
    "df_immigration = df_immigration.where(col(\"cicid\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration.printSchema()\n",
    "#cast double columns to int\n",
    "df_immigration = df_immigration.withColumn(\"year\", df_immigration[\"year\"].cast(Int()))\n",
    "df_immigration = df_immigration.withColumn(\"month\", df_immigration[\"month\"].cast(Int()))\n",
    "df_immigration = df_immigration.withColumn(\"city_code_origin\", df_immigration[\"city_code_origin\"].cast(Int()))\n",
    "df_immigration = df_immigration.withColumn(\"country_code_residence\", df_immigration[\"country_code_residence\"].cast(Int()))\n",
    "df_immigration = df_immigration.withColumn(\"travel_code\", df_immigration[\"travel_code\"].cast(Int()))\n",
    "df_immigration = df_immigration.withColumn(\"visa_code\", df_immigration[\"visa_code\"].cast(Int()))\n",
    "df_immigration = df_immigration.withColumn(\"birth_year\", df_immigration[\"birth_year\"].cast(Int()))\n",
    "df_immigration.printSchema()\n",
    "df_immigration.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_codes_to_dict(string, separator):\n",
    "    dictionary = {}\n",
    "    for line in string.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        #split into code and country description\n",
    "        l = line.split(separator) #.strip()\n",
    "        #save in dicctionary\n",
    "        string = dict(zip(l[::2], l[1::2]))\n",
    "        dictionary.update(string)\n",
    "        #strip leading\n",
    "        #print(dictionary)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get city and residence country codes and description\n",
    "#I94CIT & I94RES\n",
    "country_codes= \"\"\"\n",
    "   582 =  'MEXICO Air Sea, and Not Reported (I-94, no land arrivals)'\n",
    "   236 =  'AFGHANISTAN'\n",
    "   101 =  'ALBANIA'\n",
    "   316 =  'ALGERIA'\n",
    "   102 =  'ANDORRA'\n",
    "   324 =  'ANGOLA'\n",
    "   529 =  'ANGUILLA'\n",
    "   518 =  'ANTIGUA-BARBUDA'\n",
    "   687 =  'ARGENTINA '\n",
    "   151 =  'ARMENIA'\n",
    "   532 =  'ARUBA'\n",
    "   438 =  'AUSTRALIA'\n",
    "   103 =  'AUSTRIA'\n",
    "   152 =  'AZERBAIJAN'\n",
    "   512 =  'BAHAMAS'\n",
    "   298 =  'BAHRAIN'\n",
    "   274 =  'BANGLADESH'\n",
    "   513 =  'BARBADOS'\n",
    "   104 =  'BELGIUM'\n",
    "   581 =  'BELIZE'\n",
    "   386 =  'BENIN'\n",
    "   509 =  'BERMUDA'\n",
    "   153 =  'BELARUS'\n",
    "   242 =  'BHUTAN'\n",
    "   688 =  'BOLIVIA'\n",
    "   717 =  'BONAIRE, ST EUSTATIUS, SABA' \n",
    "   164 =  'BOSNIA-HERZEGOVINA'\n",
    "   336 =  'BOTSWANA'\n",
    "   689 =  'BRAZIL'\n",
    "   525 =  'BRITISH VIRGIN ISLANDS'\n",
    "   217 =  'BRUNEI'\n",
    "   105 =  'BULGARIA'\n",
    "   393 =  'BURKINA FASO'\n",
    "   243 =  'BURMA'\n",
    "   375 =  'BURUNDI'\n",
    "   310 =  'CAMEROON'\n",
    "   326 =  'CAPE VERDE'\n",
    "   526 =  'CAYMAN ISLANDS'\n",
    "   383 =  'CENTRAL AFRICAN REPUBLIC'\n",
    "   384 =  'CHAD'\n",
    "   690 =  'CHILE'\n",
    "   245 =  'CHINA, PRC'\n",
    "   721 =  'CURACAO' \n",
    "   270 =  'CHRISTMAS ISLAND'\n",
    "   271 =  'COCOS ISLANDS'\n",
    "   691 =  'COLOMBIA'\n",
    "   317 =  'COMOROS'\n",
    "   385 =  'CONGO'\n",
    "   467 =  'COOK ISLANDS'\n",
    "   575 =  'COSTA RICA'\n",
    "   165 =  'CROATIA'\n",
    "   584 =  'CUBA'\n",
    "   218 =  'CYPRUS'\n",
    "   140 =  'CZECH REPUBLIC'\n",
    "   723 =  'FAROE ISLANDS (PART OF DENMARK)'  \n",
    "   108 =  'DENMARK'\n",
    "   322 =  'DJIBOUTI'\n",
    "   519 =  'DOMINICA'\n",
    "   585 =  'DOMINICAN REPUBLIC'\n",
    "   240 =  'EAST TIMOR'\n",
    "   692 =  'ECUADOR'\n",
    "   368 =  'EGYPT'\n",
    "   576 =  'EL SALVADOR'\n",
    "   399 =  'EQUATORIAL GUINEA'\n",
    "   372 =  'ERITREA'\n",
    "   109 =  'ESTONIA'\n",
    "   369 =  'ETHIOPIA'\n",
    "   604 =  'FALKLAND ISLANDS'\n",
    "   413 =  'FIJI'\n",
    "   110 =  'FINLAND'\n",
    "   111 =  'FRANCE'\n",
    "   601 =  'FRENCH GUIANA'\n",
    "   411 =  'FRENCH POLYNESIA'\n",
    "   387 =  'GABON'\n",
    "   338 =  'GAMBIA'\n",
    "   758 =  'GAZA STRIP' \n",
    "   154 =  'GEORGIA'\n",
    "   112 =  'GERMANY'\n",
    "   339 =  'GHANA'\n",
    "   143 =  'GIBRALTAR'\n",
    "   113 =  'GREECE'\n",
    "   520 =  'GRENADA'\n",
    "   507 =  'GUADELOUPE'\n",
    "   577 =  'GUATEMALA'\n",
    "   382 =  'GUINEA'\n",
    "   327 =  'GUINEA-BISSAU'\n",
    "   603 =  'GUYANA'\n",
    "   586 =  'HAITI'\n",
    "   726 =  'HEARD AND MCDONALD IS.'\n",
    "   149 =  'HOLY SEE/VATICAN'\n",
    "   528 =  'HONDURAS'\n",
    "   206 =  'HONG KONG'\n",
    "   114 =  'HUNGARY'\n",
    "   115 =  'ICELAND'\n",
    "   213 =  'INDIA'\n",
    "   759 =  'INDIAN OCEAN AREAS (FRENCH)' \n",
    "   729 =  'INDIAN OCEAN TERRITORY' \n",
    "   204 =  'INDONESIA'\n",
    "   249 =  'IRAN'\n",
    "   250 =  'IRAQ'\n",
    "   116 =  'IRELAND'\n",
    "   251 =  'ISRAEL'\n",
    "   117 =  'ITALY'\n",
    "   388 =  'IVORY COAST'\n",
    "   514 =  'JAMAICA'\n",
    "   209 =  'JAPAN'\n",
    "   253 =  'JORDAN'\n",
    "   201 =  'KAMPUCHEA'\n",
    "   155 =  'KAZAKHSTAN'\n",
    "   340 =  'KENYA'\n",
    "   414 =  'KIRIBATI'\n",
    "   732 =  'KOSOVO' \n",
    "   272 =  'KUWAIT'\n",
    "   156 =  'KYRGYZSTAN'\n",
    "   203 =  'LAOS'\n",
    "   118 =  'LATVIA'\n",
    "   255 =  'LEBANON'\n",
    "   335 =  'LESOTHO'\n",
    "   370 =  'LIBERIA'\n",
    "   381 =  'LIBYA'\n",
    "   119 =  'LIECHTENSTEIN'\n",
    "   120 =  'LITHUANIA'\n",
    "   121 =  'LUXEMBOURG'\n",
    "   214 =  'MACAU'\n",
    "   167 =  'MACEDONIA'\n",
    "   320 =  'MADAGASCAR'\n",
    "   345 =  'MALAWI'\n",
    "   273 =  'MALAYSIA'\n",
    "   220 =  'MALDIVES'\n",
    "   392 =  'MALI'\n",
    "   145 =  'MALTA'\n",
    "   472 =  'MARSHALL ISLANDS'\n",
    "   511 =  'MARTINIQUE'\n",
    "   389 =  'MAURITANIA'\n",
    "   342 =  'MAURITIUS'\n",
    "   760 =  'MAYOTTE (AFRICA - FRENCH)' \n",
    "   473 =  'MICRONESIA, FED. STATES OF'\n",
    "   157 =  'MOLDOVA'\n",
    "   122 =  'MONACO'\n",
    "   299 =  'MONGOLIA'\n",
    "   735 =  'MONTENEGRO' \n",
    "   521 =  'MONTSERRAT'\n",
    "   332 =  'MOROCCO'\n",
    "   329 =  'MOZAMBIQUE'\n",
    "   371 =  'NAMIBIA'\n",
    "   440 =  'NAURU'\n",
    "   257 =  'NEPAL'\n",
    "   123 =  'NETHERLANDS'\n",
    "   508 =  'NETHERLANDS ANTILLES'\n",
    "   409 =  'NEW CALEDONIA'\n",
    "   464 =  'NEW ZEALAND'\n",
    "   579 =  'NICARAGUA'\n",
    "   390 =  'NIGER'\n",
    "   343 =  'NIGERIA'\n",
    "   470 =  'NIUE'\n",
    "   275 =  'NORTH KOREA'\n",
    "   124 =  'NORWAY'\n",
    "   256 =  'OMAN'\n",
    "   258 =  'PAKISTAN'\n",
    "   474 =  'PALAU'\n",
    "   743 =  'PALESTINE' \n",
    "   504 =  'PANAMA'\n",
    "   441 =  'PAPUA NEW GUINEA'\n",
    "   693 =  'PARAGUAY'\n",
    "   694 =  'PERU'\n",
    "   260 =  'PHILIPPINES'\n",
    "   416 =  'PITCAIRN ISLANDS'\n",
    "   107 =  'POLAND'\n",
    "   126 =  'PORTUGAL'\n",
    "   297 =  'QATAR'\n",
    "   748 =  'REPUBLIC OF SOUTH SUDAN'\n",
    "   321 =  'REUNION'\n",
    "   127 =  'ROMANIA'\n",
    "   158 =  'RUSSIA'\n",
    "   376 =  'RWANDA'\n",
    "   128 =  'SAN MARINO'\n",
    "   330 =  'SAO TOME AND PRINCIPE'\n",
    "   261 =  'SAUDI ARABIA'\n",
    "   391 =  'SENEGAL'\n",
    "   142 =  'SERBIA AND MONTENEGRO'\n",
    "   745 =  'SERBIA' \n",
    "   347 =  'SEYCHELLES'\n",
    "   348 =  'SIERRA LEONE'\n",
    "   207 =  'SINGAPORE'\n",
    "   141 =  'SLOVAKIA'\n",
    "   166 =  'SLOVENIA'\n",
    "   412 =  'SOLOMON ISLANDS'\n",
    "   397 =  'SOMALIA'\n",
    "   373 =  'SOUTH AFRICA'\n",
    "   276 =  'SOUTH KOREA'\n",
    "   129 =  'SPAIN'\n",
    "   244 =  'SRI LANKA'\n",
    "   346 =  'ST. HELENA'\n",
    "   522 =  'ST. KITTS-NEVIS'\n",
    "   523 =  'ST. LUCIA'\n",
    "   502 =  'ST. PIERRE AND MIQUELON'\n",
    "   524 =  'ST. VINCENT-GRENADINES'\n",
    "   716 =  'SAINT BARTHELEMY' \n",
    "   736 =  'SAINT MARTIN' \n",
    "   749 =  'SAINT MAARTEN' \n",
    "   350 =  'SUDAN'\n",
    "   602 =  'SURINAME'\n",
    "   351 =  'SWAZILAND'\n",
    "   130 =  'SWEDEN'\n",
    "   131 =  'SWITZERLAND'\n",
    "   262 =  'SYRIA'\n",
    "   268 =  'TAIWAN'\n",
    "   159 =  'TAJIKISTAN'\n",
    "   353 =  'TANZANIA'\n",
    "   263 =  'THAILAND'\n",
    "   304 =  'TOGO'\n",
    "   417 =  'TONGA'\n",
    "   516 =  'TRINIDAD AND TOBAGO'\n",
    "   323 =  'TUNISIA'\n",
    "   264 =  'TURKEY'\n",
    "   161 =  'TURKMENISTAN'\n",
    "   527 =  'TURKS AND CAICOS ISLANDS'\n",
    "   420 =  'TUVALU'\n",
    "   352 =  'UGANDA'\n",
    "   162 =  'UKRAINE'\n",
    "   296 =  'UNITED ARAB EMIRATES'\n",
    "   135 =  'UNITED KINGDOM'\n",
    "   695 =  'URUGUAY'\n",
    "   163 =  'UZBEKISTAN'\n",
    "   410 =  'VANUATU'\n",
    "   696 =  'VENEZUELA'\n",
    "   266 =  'VIETNAM'\n",
    "   469 =  'WALLIS AND FUTUNA ISLANDS'\n",
    "   757 =  'WEST INDIES (FRENCH)' \n",
    "   333 =  'WESTERN SAHARA'\n",
    "   465 =  'WESTERN SAMOA'\n",
    "   216 =  'YEMEN'\n",
    "   139 =  'YUGOSLAVIA'\n",
    "   301 =  'ZAIRE'\n",
    "   344 =  'ZAMBIA'\n",
    "   315 =  'ZIMBABWE'\n",
    "   403 =  'INVALID: AMERICAN SAMOA'\n",
    "   712 =  'INVALID: ANTARCTICA' \n",
    "   700 =  'INVALID: BORN ON BOARD SHIP'\n",
    "   719 =  'INVALID: BOUVET ISLAND (ANTARCTICA/NORWAY TERR.)'\n",
    "   574 =  'INVALID: CANADA'\n",
    "   720 =  'INVALID: CANTON AND ENDERBURY ISLS' \n",
    "   106 =  'INVALID: CZECHOSLOVAKIA'\n",
    "   739 =  'INVALID: DRONNING MAUD LAND (ANTARCTICA-NORWAY)' \n",
    "   394 =  'INVALID: FRENCH SOUTHERN AND ANTARCTIC'\n",
    "   501 =  'INVALID: GREENLAND'\n",
    "   404 =  'INVALID: GUAM'\n",
    "   730 =  'INVALID: INTERNATIONAL WATERS' \n",
    "   731 =  'INVALID: JOHNSON ISLAND' \n",
    "   471 =  'INVALID: MARIANA ISLANDS, NORTHERN'\n",
    "   737 =  'INVALID: MIDWAY ISLANDS' \n",
    "   753 =  'INVALID: MINOR OUTLYING ISLANDS - USA'\n",
    "   740 =  'INVALID: NEUTRAL ZONE (S. ARABIA/IRAQ)' \n",
    "   710 =  'INVALID: NON-QUOTA IMMIGRANT'\n",
    "   505 =  'INVALID: PUERTO RICO'\n",
    "    0  =  'INVALID: STATELESS'\n",
    "   705 =  'INVALID: STATELESS'\n",
    "   583 =  'INVALID: UNITED STATES'\n",
    "   407 =  'INVALID: UNITED STATES'\n",
    "   999 =  'INVALID: UNKNOWN'\n",
    "   239 =  'INVALID: UNKNOWN COUNTRY'\n",
    "   134 =  'INVALID: USSR'\n",
    "   506 =  'INVALID: U.S. VIRGIN ISLANDS'\n",
    "   755 =  'INVALID: WAKE ISLAND'  \n",
    "   311 =  'Collapsed Tanzania (should not show)'\n",
    "   741 =  'Collapsed Curacao (should not show)'\n",
    "    54 =  'No Country Code (54)'\n",
    "   100 =  'No Country Code (100)'\n",
    "   187 =  'No Country Code (187)'\n",
    "   190 =  'No Country Code (190)'\n",
    "   200 =  'No Country Code (200)'\n",
    "   219 =  'No Country Code (219)'\n",
    "   238 =  'No Country Code (238)'\n",
    "   277 =  'No Country Code (277)'\n",
    "   293 =  'No Country Code (293)'\n",
    "   300 =  'No Country Code (300)'\n",
    "   319 =  'No Country Code (319)'\n",
    "   365 =  'No Country Code (365)'\n",
    "   395 =  'No Country Code (395)'\n",
    "   400 =  'No Country Code (400)'\n",
    "   485 =  'No Country Code (485)'\n",
    "   503 =  'No Country Code (503)'\n",
    "   589 =  'No Country Code (589)'\n",
    "   592 =  'No Country Code (592)'\n",
    "   791 =  'No Country Code (791)'\n",
    "   849 =  'No Country Code (849)'\n",
    "   914 =  'No Country Code (914)'\n",
    "   944 =  'No Country Code (944)'\n",
    "   996 =  'No Country Code (996)'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove quotes\n",
    "country_codes = country_codes.replace(\"'\",'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dictionary to data frame to be able to convert it to parquet file\n",
    "#https://stackoverflow.com/questions/61339594/how-to-convert-a-dictionary-to-dataframe-in-pyspark\n",
    "data_country = {}\n",
    "\n",
    "data_country = split_codes_to_dict(country_codes, \" =  \")\n",
    "df_country_code = spark.createDataFrame(data_country.items(), \n",
    "                      schema=R(fields=[\n",
    "                          Fld(\"country_code\", Str()), \n",
    "                          Fld(\"country_name\", Str())]))\n",
    "\n",
    "df_country_code = df_country_code.withColumn(\"country_code\", df_country_code[\"country_code\"].cast(Int()))\n",
    "df_country_code.show()\n",
    "df_country_code.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete nulls in country_code since it will be the primary key. No null values expected\n",
    "df_country_code = df_country_code.where(col(\"country_code\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#travel code and description and remove quotes\n",
    "travel_code = \"\"\"\n",
    "   1 = 'Air'\n",
    "   2 = 'Sea'\n",
    "   3 = 'Land'\n",
    "   9 = 'Not reported'\"\"\".replace(\"'\",'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dictionary to data frame to be able to convert it to parquet file\n",
    "data_travel = {}\n",
    "\n",
    "data_travel = split_codes_to_dict(travel_code, \" = \")\n",
    "df_travel_code = spark.createDataFrame(data_travel.items(), \n",
    "                      schema=R(fields=[\n",
    "                          Fld(\"travel_code\", Str()), \n",
    "                          Fld(\"travel_name\", Str())]))\n",
    "\n",
    "df_travel_code = df_travel_code.withColumn(\"travel_code\", df_travel_code[\"travel_code\"].cast(Int()))\n",
    "df_travel_code.show()\n",
    "df_travel_code.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete nulls in travel_code since it will be the primary key. No null values expected\n",
    "df_travel_code = df_travel_code.where(col(\"travel_code\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visa_codes = \"\"\"\n",
    "   1 = 'Business'\n",
    "   2 = 'Pleasure'\n",
    "   3 = 'Student'\n",
    "\"\"\".replace(\"'\",'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dictionary to data frame to be able to convert it to parquet file\n",
    "data_visa = {}\n",
    "\n",
    "data_visa = split_codes_to_dict(visa_codes, \" = \")\n",
    "df_visa_code = spark.createDataFrame(data_visa.items(), \n",
    "                      schema=R(fields=[\n",
    "                          Fld(\"visa_code\", Str()), \n",
    "                          Fld(\"visa_name\", Str())]))\n",
    "\n",
    "df_visa_code = df_visa_code.withColumn(\"visa_code\", df_visa_code[\"visa_code\"].cast(Int()))\n",
    "df_visa_code.show()\n",
    "df_visa_code.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete nulls in visa_code since it will be the primary key. No null values expected\n",
    "df_visa_code = df_visa_code.where(col(\"visa_code\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of rows per dataframe to compare to redshift load\n",
    "print((df_immigration.count(), len(df_immigration.columns)))\n",
    "print((df_airport.count(), len(df_airport.columns)))\n",
    "print((df_us_demograhics.count(), len(df_us_demograhics.columns)))\n",
    "print((df_country_code.count(), len(df_country_code.columns)))\n",
    "print((df_travel_code.count(), len(df_travel_code.columns)))\n",
    "print((df_visa_code.count(), len(df_visa_code.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to S3\n",
    "#output_data = \"output/\" #create your own bucket\n",
    "output_data = config.get('S3', 'output_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_demograhics.write.mode('overwrite').parquet(output_data + \"us_demographics/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration.write.mode('overwrite').parquet(output_data + \"us_immigration/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport.write.mode('overwrite').parquet(output_data + \"airport/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country_code.write.mode('overwrite').parquet(output_data + \"country_code/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_travel_code.write.mode('overwrite').parquet(output_data + \"travel_code/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visa_code.write.mode('overwrite').parquet(output_data + \"visa_code/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "As mentioned earlier, the goal of this project is to provide high quality data to allow for a variety of analyses and reports while making it as easy as possible for the user to interact with the data. Furthermore, the data will be updated in batches and does not consitute transactional data. Hence a data warehouse and a relational database setup was chosen since it provides structured data for query and analysis using a understandable and performant dimensional model facilitating analyses.\n",
    "\n",
    "A modified version of the start schema is used to model the data since analyses on travel data is the goal. The star schema simplifies queries and allows for fast aggregations which fit the goals of this project. There are two fact tables and five dimensional tables. A visual representation of the data model can be found in the README file.\n",
    "\n",
    "Primary and foreign keys are defined in the tables even though Redshift does not enforce them since the query optimizer uses those constraints to generate more efficient query plans.\n",
    "\n",
    "DISTKEY and SORTKEY are set to optimize data distribution and query processing. The choice of DISTSTYLE is left to Amazon Redshift as it is best practice.\n",
    "\n",
    "The six staging tables that handle load from S3 buckets to increase efficiency of ETL processes and ensure data integrity.\n",
    "\n",
    "The two fact tables are:\n",
    "\n",
    "#### us_immigration\n",
    "This table contains all information available on visitors to the US include travel, visa and country codes. The column 'cicid' is the primary key for this table. Foreign keys are set to illustrate relations to dimension tables. Not null constraints are added to foreign keys. The column 'state_code_residence' was chosen as a DISTKEY and SORTKEY since it is used to link to states and municipalties and will be joined with those tables often.\n",
    "\n",
    "#### us_municipality\n",
    "This table holds facts about a US municipalities such as total population and average household size. The primary key is a composite of city and state_code since the city name must not be unique. Foreign key is set to 'state_code' to illustrate the relation to table 'us_states'. The column 'state_code' was chosen as a DISTKEY and SORTKEY since it is used to link to 'us_states' and will be joined with this table often. This ensures that data with same value in 'state_code' are on the same slice speeding up queries. Since the cities in this table do not align with cities in 'us_immigration' or 'airport', it is not linked to them.\n",
    "\n",
    "The dimension tables are:\n",
    "\n",
    "#### us_states\n",
    "This table contains all state codes and state names of the US. Primary key is 'state_code'. The column 'state_code' was chosen as DISTKEY and SORTKEY since it is the DISTKEY for referenced tables 'us_immigration' and 'us_municipality'. This way data that is joined is on the same slice.\n",
    "\n",
    "#### travel_code\n",
    "This table contains information on the mode of transport used by the visitor entering the US. 'travel_code' is the primary and distribution key since it is unique and used to join table to 'us_immigration'.\n",
    "\n",
    "#### visa_code\n",
    "This table contains information on the type of visa the visitor used. 'visa_code' is the primary and distribution key since it is unique and used to join table to 'us_immigration'.\n",
    "\n",
    "#### airport\n",
    "This table holds information on US airports. The column 'iata_code' is the primary since it is by definition unique. The column 'state_code' was chosen as a DISTKEY and SORTKEY since it is used to link to 'us_immigration' and will be joined with this table often. 'city_code_destination' is not the DISTKEY in table 'us_immigration' hence making 'iata_code' the DISTKEY does not yield any efficiencies.\n",
    "\n",
    "and distribution key since it is by definiton unique and used to join table to 'us_immigration'.\n",
    "\n",
    "#### country\n",
    "This table contains country codes and its names. The are the countries visitors to the US are coming from. 'country_code' is the primary and distribution key since it is unique and used to join table to 'us_immigration'.\n",
    "\n",
    "The staging tables are:\n",
    "\n",
    "staging_airport\n",
    "staging_us_demographics\n",
    "staging_us_immigration\n",
    "staging_country_code\n",
    "staging_visa_code\n",
    "staging_travel_code\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "1. Load datasets:\n",
    "The data files that are available as csv are loaded from the workspace into pyspark dataframes. Immigration data is provided in sas7bdat file format and saved on disk. The path is the following: ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat.\n",
    "2. Clean data\n",
    "Ensure primary keys are not null since Redshift does not enforce key constraints\n",
    "3. Write data as parquet files to S3 buckets\n",
    "4. Drop tables on Redshift if necessary\n",
    "5. Create tables on Redshift\n",
    "6. Copy data from S3 into staging tables\n",
    "7. Insert data from staging tables to final tables\n",
    "8. Run data quality checks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here\n",
    "import psycopg2\n",
    "import configparser\n",
    "from sql_queries import create_table_queries, drop_table_queries, copy_table_queries, insert_table_queries, data_quality_count_check, data_quality_null_references_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_tables(cur, conn):\n",
    "    \"\"\"\n",
    "        Load data in parquet files from S3 to staging tables on Redshift specified in copy_table_queries \n",
    "        \n",
    "        Arguments:\n",
    "            cur - PostgreSQL cursor object\n",
    "            conn - psycopg2 connection instance\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    for query in drop_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tables(cur, conn):\n",
    "    \"\"\"\n",
    "        Load data in parquet files from S3 to staging tables on Redshift specified in copy_table_queries \n",
    "        \n",
    "        Arguments:\n",
    "            cur - PostgreSQL cursor object\n",
    "            conn - psycopg2 connection instance\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    for query in create_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_staging_tables(cur, conn):\n",
    "    \"\"\"\n",
    "        Load data in parquet files from S3 to staging tables on Redshift specified in copy_table_queries \n",
    "        \n",
    "        Arguments:\n",
    "            cur - PostgreSQL cursor object\n",
    "            conn - psycopg2 connection instance\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    for query in copy_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_tables(cur, conn):\n",
    "    \"\"\"\n",
    "        Load data from staging tables to analytics tables specified in insert_table_queries\n",
    "        \n",
    "        Arguments:\n",
    "            cur - PostgreSQL cursor object\n",
    "            conn - psycopg2 connection instance\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    for query in insert_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "#run quality checks to see how many records exist in each redshift table\n",
    "def count_check(cur, conn):\n",
    "    for query in data_quality_count_check:\n",
    "        cur.execute(query)\n",
    "        output = cur.fetchall()\n",
    "        print(\"{} has {} records\".format(query.split(' ')[-1], output[0][0]))\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "#run quality checks to see how many records exist in each redshift table\n",
    "def null_reference_check(cur, conn):\n",
    "    for query in data_quality_null_references_check:\n",
    "        cur.execute(query)\n",
    "        output = cur.fetchall()\n",
    "        print(\"table {}\".format(query.split(' ')[3], output[0][0]))\n",
    "        print(\"column {} has {} null references\".format(query.split(' ')[1], output[0][0]))\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "cur = conn.cursor()\n",
    "\n",
    "drop_tables(cur, conn)\n",
    "create_tables(cur, conn)\n",
    "load_staging_tables(cur, conn)\n",
    "insert_tables(cur, conn)\n",
    "#count_check(cur, conn)\n",
    "#null_reference_check(cur, conn)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "* To ensure that the pipeline ran as expected the count of the tables can be compared to the count of the dataframes before writing them to s3.\n",
    "* The second data quality check outputs the number of null references for foreign keys on tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "cur = conn.cursor()\n",
    "\n",
    "count_check(cur, conn)\n",
    "null_reference_check(cur, conn)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rationale for the choice of tools and technologies for the project\n",
    "Apache Spark is used to clean and write the data to S3 since the datasets the project works with are middle to large-sized and Apache Spark is a fast engine for large-scale data processing. Furthermore, it can be used under its open source license.\n",
    "Data is loaded into S3 buckets since it consitutes low-cost storage and provides the possibility to be loaded into a data lake if need be. \n",
    "Processed data is stored in Apache Parquet format since its provides efficient columnar data storage and compression and works well in combination with Redshift. Additionally, it is licensed under the Apache software foundation and available to any project\n",
    "Redshift is used since fewer skills are needed than when using e.g. Apache Spark. Any user with basic SQL knowledge can get value from the data by using OLAP queries on Redshift. Furthermore, Redshift is scalable and expenses can be controlled by spinning clusters up or down as needed.\n",
    "Node type DC2 is used for Redshift over DC1 since these are the same price as DC1 instances of the same size, but I/O is significantly higher.\n",
    "\n",
    "### Update frequency\n",
    "I94 immigration data is published on a monthly basis and should be updated every month to provide the latest data to the user. Codes used in the data set should be updated from the SAS descriptions if necessary.\n",
    "U.S. City Demographic Data data is published on a yearly basis and should be updated once new data is published by the US Census Bureau.\n",
    "Airport dataset should be updated with every load of I94 immigration data to ensure data quality\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people.\n",
    "\n",
    "### Scenario: data increased by 100x\n",
    "The RAM to process data in the current fashion would probably not be enough. The cleaning process can be run on an Apache Spark cluster that parallelizes the task across the RAM of multiple machines. \n",
    "Increasing the number of prefixe in an Amazon S3 buckets enables us to parallelize reads since S3 can achieve at least 3,500 PUT/COPY/POST/DELETE requests per second per prefix in a bucket. \n",
    "It could also be considered to add more nodes on the Redshift cluster since Redshift is designed to scale out by adding nodes to the cluster. Adding nodes adds disk space as well as computing power\n",
    "\n",
    "### Scenario: data populates a dashboard that must be updated on a daily basis by 7am every day\n",
    "The ETL pipeline should be run using a tool especially designed for ETL piplines. This allows for more control over the workflow and includes scheduling and logging.\n",
    "Apache Airflow could be used separating the current pipeline into tasks run with a DAG (Directed Acyclic Graph) that performs these inter-dependent tasks. This would allow for tables to be truncated or dropped individually. The pipeline can be scheduled to run every night notifying data engineers about any failures.\n",
    "\n",
    "### Scenario: database needed to be accessed by 100+ people\n",
    "This is a strength of Amazon Redshift. Its columnar based storage system allows fast processing of aggregation queries simultaneously by large number of users. It could also be considered to add more nodes on the Redshift cluster to get more computing power"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}