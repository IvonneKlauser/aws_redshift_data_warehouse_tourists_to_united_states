{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on tourists to the United States\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "Prepare data to analyze tourist travel behavior. Query flexibility, analysis on redshift so everyone can use it -> OLAP -> Redshift\n",
    "Updated in batches\n",
    "Understandable & performant dimensional model\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, DateType as Date, TimestampType as Timestamp\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import types as T\n",
    "import pyspark.sql.functions\n",
    "from pyspark.sql.functions import udf, col, expr\n",
    "#from pyspark.sql.functions import col\n",
    "#from pyspark.sql.functions import expr\n",
    "import os\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the config object and read cfg file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "#Accessing the AWS user IAM credentials in the dwh.cfg file using config object\n",
    "os.environ['KEY']=config['AWS']['KEY']\n",
    "os.environ['SECRET']=config['AWS']['SECRET']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.2\").enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigration = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_immigration.show(5)\n",
    "df_immigration.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sas date to date\n",
    "def convert_datetime(x):\n",
    "    try:\n",
    "        start = datetime(1960, 1, 1)\n",
    "        return start + timedelta(days=int(x))\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert arrival and departure date to date\n",
    "udf_datetime_from_sas = udf(lambda x: convert_datetime(x), T.DateType())\n",
    "df_immigration = df_immigration.withColumn(\"arrival_date\", udf_datetime_from_sas(\"arrdate\")).withColumn(\"departure_date\", udf_datetime_from_sas(\"depdate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that cicid is not null since it is the primary key\n",
    "df_immigration = df_immigration.dropDuplicates()\n",
    "df_immigration.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load airport data\n",
    "df_airport = spark.read.format(\"csv\").option(\"header\", True).load(\"airport-codes_csv.csv\")\n",
    "df_airport.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create song data schema to ensure that schema is inferred correctly\n",
    "demoSchema = R([\n",
    "    Fld(\"city\",Str()),\n",
    "    Fld(\"state_name\",Str()),\n",
    "    Fld(\"median_age\",Dbl()),\n",
    "    Fld(\"male_population\",Dbl()),\n",
    "    Fld(\"female_population\",Dbl()),\n",
    "    Fld(\"total_population\",Dbl()),\n",
    "    Fld(\"number of veterans\",Dbl()),\n",
    "    Fld(\"foreign_born\",Dbl()),\n",
    "    Fld(\"avg_household_size\",Dbl()),\n",
    "    Fld(\"state_code\",Str()),\n",
    "    Fld(\"race\",Str()),\n",
    "    Fld(\"count\",Dbl()),\n",
    "])\n",
    "# read song data file\n",
    "#df = spark.read.schema(songSchema).json(song_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load demographics data\n",
    "df_us_demograhics = spark.read.format(\"csv\").option(\"header\", True).option(\"delimiter\", \";\").schema(demoSchema).load(\"us-cities-demographics.csv\")\n",
    "df_us_demograhics.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_demograhics.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here airport data\n",
    "# extract columns to create songs table\n",
    "df_airport = df_airport.select('iata_code', 'name', 'iso_country','iso_region','municipality','coordinates', 'type')\n",
    "df_airport.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter blank iata_codes out since this column will be a primary key and drop duplicates\n",
    "df_airport = df_airport.filter(df_airport.iata_code != '').dropDuplicates()\n",
    "# filter to onyl US airports\n",
    "df_airport = df_airport.filter(df_airport.iso_country == 'US').dropDuplicates()\n",
    "#only airports are relevant, not helipads etc.\n",
    "df_airport = df_airport.filter(df_airport.type.contains('airport'))\n",
    "df_airport.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split iso_region in country and state\n",
    "split_col = pyspark.sql.functions.split(df_airport['iso_region'], '-')\n",
    "df_airport = df_airport.withColumn('country_code', split_col.getItem(0))\n",
    "df_airport = df_airport.withColumn('state_code', split_col.getItem(1))\n",
    "df_airport.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split coordinates into longitude and latitude\n",
    "split_col = pyspark.sql.functions.split(df_airport['coordinates'], ', ')\n",
    "df_airport = df_airport.withColumn('latitude', split_col.getItem(0))\n",
    "df_airport = df_airport.withColumn('longitude', split_col.getItem(1))\n",
    "df_airport.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport = df_airport.drop('coordinates')\n",
    "df_airport = df_airport.drop('country_code')\n",
    "df_airport.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport.printSchema()\n",
    "#cast string coordinates to double\n",
    "df_airport = df_airport.withColumn(\"latitude\", df_airport[\"latitude\"].cast(Dbl()))\n",
    "df_airport = df_airport.withColumn(\"longitude\", df_airport[\"longitude\"].cast(Dbl()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean demographics\n",
    "# select relevant columns\n",
    "df_us_demograhics = df_us_demograhics.select('city', 'state_name', 'median_age','male_population','female_population','total_population', 'foreign_born', 'avg_household_size', 'state_code').dropDuplicates()\n",
    "df_us_demograhics.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that city is not null since it is the primary key\n",
    "df_us_demograhics = df_us_demograhics.filter(df_us_demograhics.city != '')\n",
    "df_us_demograhics.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_demograhics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean immigration data\n",
    "df_immigration.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only relevant columns\n",
    "df_immigration = df_immigration.select('cicid', col(\"i94yr\").alias(\"year\"), \n",
    "                                       col(\"i94mon\").alias(\"month\"),\n",
    "                                       col(\"i94cit\").alias(\"city_code_origin\"),\n",
    "                                       col(\"i94res\").alias(\"country_code_residence\"),\n",
    "                                       col(\"i94port\").alias(\"city_code_destination\"),\n",
    "                                       col(\"arrival_date\"),\n",
    "                                       col(\"i94mode\").alias(\"travel_code\"),\n",
    "                                       col(\"i94addr\").alias(\"state_code_residence\"),\n",
    "                                       col(\"departure_date\"),\n",
    "                                       col(\"i94visa\").alias(\"visa_code\"),\n",
    "                                       col(\"biryear\").alias(\"birth_year\"),\n",
    "                                       col(\"gender\"),\n",
    "                                       col(\"airline\")\n",
    "                                      ).distinct()\n",
    "df_immigration.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete nulls in cicid since it will be the primary key. No null values expected\n",
    "df_immigration = df_immigration.where(col(\"cicid\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration.printSchema()\n",
    "#cast double columns to int\n",
    "df_immigration = df_immigration.withColumn(\"year\", df_immigration[\"year\"].cast(Int()))\n",
    "df_immigration = df_immigration.withColumn(\"month\", df_immigration[\"month\"].cast(Int()))\n",
    "df_immigration = df_immigration.withColumn(\"city_code_origin\", df_immigration[\"city_code_origin\"].cast(Int()))\n",
    "df_immigration = df_immigration.withColumn(\"country_code_residence\", df_immigration[\"country_code_residence\"].cast(Int()))\n",
    "df_immigration = df_immigration.withColumn(\"travel_code\", df_immigration[\"travel_code\"].cast(Int()))\n",
    "df_immigration = df_immigration.withColumn(\"visa_code\", df_immigration[\"visa_code\"].cast(Int()))\n",
    "df_immigration = df_immigration.withColumn(\"birth_year\", df_immigration[\"birth_year\"].cast(Int()))\n",
    "df_immigration.printSchema()\n",
    "df_immigration.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_codes_to_dict(string, separator):\n",
    "    dictionary = {}\n",
    "    for line in string.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        #split into code and country description\n",
    "        l = line.split(separator) #.strip()\n",
    "        #save in dicctionary\n",
    "        string = dict(zip(l[::2], l[1::2]))\n",
    "        dictionary.update(string)\n",
    "        #strip leading\n",
    "        #print(dictionary)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get city and residence country codes and description\n",
    "#I94CIT & I94RES\n",
    "country_codes= \"\"\"\n",
    "   582 =  'MEXICO Air Sea, and Not Reported (I-94, no land arrivals)'\n",
    "   236 =  'AFGHANISTAN'\n",
    "   101 =  'ALBANIA'\n",
    "   316 =  'ALGERIA'\n",
    "   102 =  'ANDORRA'\n",
    "   324 =  'ANGOLA'\n",
    "   529 =  'ANGUILLA'\n",
    "   518 =  'ANTIGUA-BARBUDA'\n",
    "   687 =  'ARGENTINA '\n",
    "   151 =  'ARMENIA'\n",
    "   532 =  'ARUBA'\n",
    "   438 =  'AUSTRALIA'\n",
    "   103 =  'AUSTRIA'\n",
    "   152 =  'AZERBAIJAN'\n",
    "   512 =  'BAHAMAS'\n",
    "   298 =  'BAHRAIN'\n",
    "   274 =  'BANGLADESH'\n",
    "   513 =  'BARBADOS'\n",
    "   104 =  'BELGIUM'\n",
    "   581 =  'BELIZE'\n",
    "   386 =  'BENIN'\n",
    "   509 =  'BERMUDA'\n",
    "   153 =  'BELARUS'\n",
    "   242 =  'BHUTAN'\n",
    "   688 =  'BOLIVIA'\n",
    "   717 =  'BONAIRE, ST EUSTATIUS, SABA' \n",
    "   164 =  'BOSNIA-HERZEGOVINA'\n",
    "   336 =  'BOTSWANA'\n",
    "   689 =  'BRAZIL'\n",
    "   525 =  'BRITISH VIRGIN ISLANDS'\n",
    "   217 =  'BRUNEI'\n",
    "   105 =  'BULGARIA'\n",
    "   393 =  'BURKINA FASO'\n",
    "   243 =  'BURMA'\n",
    "   375 =  'BURUNDI'\n",
    "   310 =  'CAMEROON'\n",
    "   326 =  'CAPE VERDE'\n",
    "   526 =  'CAYMAN ISLANDS'\n",
    "   383 =  'CENTRAL AFRICAN REPUBLIC'\n",
    "   384 =  'CHAD'\n",
    "   690 =  'CHILE'\n",
    "   245 =  'CHINA, PRC'\n",
    "   721 =  'CURACAO' \n",
    "   270 =  'CHRISTMAS ISLAND'\n",
    "   271 =  'COCOS ISLANDS'\n",
    "   691 =  'COLOMBIA'\n",
    "   317 =  'COMOROS'\n",
    "   385 =  'CONGO'\n",
    "   467 =  'COOK ISLANDS'\n",
    "   575 =  'COSTA RICA'\n",
    "   165 =  'CROATIA'\n",
    "   584 =  'CUBA'\n",
    "   218 =  'CYPRUS'\n",
    "   140 =  'CZECH REPUBLIC'\n",
    "   723 =  'FAROE ISLANDS (PART OF DENMARK)'  \n",
    "   108 =  'DENMARK'\n",
    "   322 =  'DJIBOUTI'\n",
    "   519 =  'DOMINICA'\n",
    "   585 =  'DOMINICAN REPUBLIC'\n",
    "   240 =  'EAST TIMOR'\n",
    "   692 =  'ECUADOR'\n",
    "   368 =  'EGYPT'\n",
    "   576 =  'EL SALVADOR'\n",
    "   399 =  'EQUATORIAL GUINEA'\n",
    "   372 =  'ERITREA'\n",
    "   109 =  'ESTONIA'\n",
    "   369 =  'ETHIOPIA'\n",
    "   604 =  'FALKLAND ISLANDS'\n",
    "   413 =  'FIJI'\n",
    "   110 =  'FINLAND'\n",
    "   111 =  'FRANCE'\n",
    "   601 =  'FRENCH GUIANA'\n",
    "   411 =  'FRENCH POLYNESIA'\n",
    "   387 =  'GABON'\n",
    "   338 =  'GAMBIA'\n",
    "   758 =  'GAZA STRIP' \n",
    "   154 =  'GEORGIA'\n",
    "   112 =  'GERMANY'\n",
    "   339 =  'GHANA'\n",
    "   143 =  'GIBRALTAR'\n",
    "   113 =  'GREECE'\n",
    "   520 =  'GRENADA'\n",
    "   507 =  'GUADELOUPE'\n",
    "   577 =  'GUATEMALA'\n",
    "   382 =  'GUINEA'\n",
    "   327 =  'GUINEA-BISSAU'\n",
    "   603 =  'GUYANA'\n",
    "   586 =  'HAITI'\n",
    "   726 =  'HEARD AND MCDONALD IS.'\n",
    "   149 =  'HOLY SEE/VATICAN'\n",
    "   528 =  'HONDURAS'\n",
    "   206 =  'HONG KONG'\n",
    "   114 =  'HUNGARY'\n",
    "   115 =  'ICELAND'\n",
    "   213 =  'INDIA'\n",
    "   759 =  'INDIAN OCEAN AREAS (FRENCH)' \n",
    "   729 =  'INDIAN OCEAN TERRITORY' \n",
    "   204 =  'INDONESIA'\n",
    "   249 =  'IRAN'\n",
    "   250 =  'IRAQ'\n",
    "   116 =  'IRELAND'\n",
    "   251 =  'ISRAEL'\n",
    "   117 =  'ITALY'\n",
    "   388 =  'IVORY COAST'\n",
    "   514 =  'JAMAICA'\n",
    "   209 =  'JAPAN'\n",
    "   253 =  'JORDAN'\n",
    "   201 =  'KAMPUCHEA'\n",
    "   155 =  'KAZAKHSTAN'\n",
    "   340 =  'KENYA'\n",
    "   414 =  'KIRIBATI'\n",
    "   732 =  'KOSOVO' \n",
    "   272 =  'KUWAIT'\n",
    "   156 =  'KYRGYZSTAN'\n",
    "   203 =  'LAOS'\n",
    "   118 =  'LATVIA'\n",
    "   255 =  'LEBANON'\n",
    "   335 =  'LESOTHO'\n",
    "   370 =  'LIBERIA'\n",
    "   381 =  'LIBYA'\n",
    "   119 =  'LIECHTENSTEIN'\n",
    "   120 =  'LITHUANIA'\n",
    "   121 =  'LUXEMBOURG'\n",
    "   214 =  'MACAU'\n",
    "   167 =  'MACEDONIA'\n",
    "   320 =  'MADAGASCAR'\n",
    "   345 =  'MALAWI'\n",
    "   273 =  'MALAYSIA'\n",
    "   220 =  'MALDIVES'\n",
    "   392 =  'MALI'\n",
    "   145 =  'MALTA'\n",
    "   472 =  'MARSHALL ISLANDS'\n",
    "   511 =  'MARTINIQUE'\n",
    "   389 =  'MAURITANIA'\n",
    "   342 =  'MAURITIUS'\n",
    "   760 =  'MAYOTTE (AFRICA - FRENCH)' \n",
    "   473 =  'MICRONESIA, FED. STATES OF'\n",
    "   157 =  'MOLDOVA'\n",
    "   122 =  'MONACO'\n",
    "   299 =  'MONGOLIA'\n",
    "   735 =  'MONTENEGRO' \n",
    "   521 =  'MONTSERRAT'\n",
    "   332 =  'MOROCCO'\n",
    "   329 =  'MOZAMBIQUE'\n",
    "   371 =  'NAMIBIA'\n",
    "   440 =  'NAURU'\n",
    "   257 =  'NEPAL'\n",
    "   123 =  'NETHERLANDS'\n",
    "   508 =  'NETHERLANDS ANTILLES'\n",
    "   409 =  'NEW CALEDONIA'\n",
    "   464 =  'NEW ZEALAND'\n",
    "   579 =  'NICARAGUA'\n",
    "   390 =  'NIGER'\n",
    "   343 =  'NIGERIA'\n",
    "   470 =  'NIUE'\n",
    "   275 =  'NORTH KOREA'\n",
    "   124 =  'NORWAY'\n",
    "   256 =  'OMAN'\n",
    "   258 =  'PAKISTAN'\n",
    "   474 =  'PALAU'\n",
    "   743 =  'PALESTINE' \n",
    "   504 =  'PANAMA'\n",
    "   441 =  'PAPUA NEW GUINEA'\n",
    "   693 =  'PARAGUAY'\n",
    "   694 =  'PERU'\n",
    "   260 =  'PHILIPPINES'\n",
    "   416 =  'PITCAIRN ISLANDS'\n",
    "   107 =  'POLAND'\n",
    "   126 =  'PORTUGAL'\n",
    "   297 =  'QATAR'\n",
    "   748 =  'REPUBLIC OF SOUTH SUDAN'\n",
    "   321 =  'REUNION'\n",
    "   127 =  'ROMANIA'\n",
    "   158 =  'RUSSIA'\n",
    "   376 =  'RWANDA'\n",
    "   128 =  'SAN MARINO'\n",
    "   330 =  'SAO TOME AND PRINCIPE'\n",
    "   261 =  'SAUDI ARABIA'\n",
    "   391 =  'SENEGAL'\n",
    "   142 =  'SERBIA AND MONTENEGRO'\n",
    "   745 =  'SERBIA' \n",
    "   347 =  'SEYCHELLES'\n",
    "   348 =  'SIERRA LEONE'\n",
    "   207 =  'SINGAPORE'\n",
    "   141 =  'SLOVAKIA'\n",
    "   166 =  'SLOVENIA'\n",
    "   412 =  'SOLOMON ISLANDS'\n",
    "   397 =  'SOMALIA'\n",
    "   373 =  'SOUTH AFRICA'\n",
    "   276 =  'SOUTH KOREA'\n",
    "   129 =  'SPAIN'\n",
    "   244 =  'SRI LANKA'\n",
    "   346 =  'ST. HELENA'\n",
    "   522 =  'ST. KITTS-NEVIS'\n",
    "   523 =  'ST. LUCIA'\n",
    "   502 =  'ST. PIERRE AND MIQUELON'\n",
    "   524 =  'ST. VINCENT-GRENADINES'\n",
    "   716 =  'SAINT BARTHELEMY' \n",
    "   736 =  'SAINT MARTIN' \n",
    "   749 =  'SAINT MAARTEN' \n",
    "   350 =  'SUDAN'\n",
    "   602 =  'SURINAME'\n",
    "   351 =  'SWAZILAND'\n",
    "   130 =  'SWEDEN'\n",
    "   131 =  'SWITZERLAND'\n",
    "   262 =  'SYRIA'\n",
    "   268 =  'TAIWAN'\n",
    "   159 =  'TAJIKISTAN'\n",
    "   353 =  'TANZANIA'\n",
    "   263 =  'THAILAND'\n",
    "   304 =  'TOGO'\n",
    "   417 =  'TONGA'\n",
    "   516 =  'TRINIDAD AND TOBAGO'\n",
    "   323 =  'TUNISIA'\n",
    "   264 =  'TURKEY'\n",
    "   161 =  'TURKMENISTAN'\n",
    "   527 =  'TURKS AND CAICOS ISLANDS'\n",
    "   420 =  'TUVALU'\n",
    "   352 =  'UGANDA'\n",
    "   162 =  'UKRAINE'\n",
    "   296 =  'UNITED ARAB EMIRATES'\n",
    "   135 =  'UNITED KINGDOM'\n",
    "   695 =  'URUGUAY'\n",
    "   163 =  'UZBEKISTAN'\n",
    "   410 =  'VANUATU'\n",
    "   696 =  'VENEZUELA'\n",
    "   266 =  'VIETNAM'\n",
    "   469 =  'WALLIS AND FUTUNA ISLANDS'\n",
    "   757 =  'WEST INDIES (FRENCH)' \n",
    "   333 =  'WESTERN SAHARA'\n",
    "   465 =  'WESTERN SAMOA'\n",
    "   216 =  'YEMEN'\n",
    "   139 =  'YUGOSLAVIA'\n",
    "   301 =  'ZAIRE'\n",
    "   344 =  'ZAMBIA'\n",
    "   315 =  'ZIMBABWE'\n",
    "   403 =  'INVALID: AMERICAN SAMOA'\n",
    "   712 =  'INVALID: ANTARCTICA' \n",
    "   700 =  'INVALID: BORN ON BOARD SHIP'\n",
    "   719 =  'INVALID: BOUVET ISLAND (ANTARCTICA/NORWAY TERR.)'\n",
    "   574 =  'INVALID: CANADA'\n",
    "   720 =  'INVALID: CANTON AND ENDERBURY ISLS' \n",
    "   106 =  'INVALID: CZECHOSLOVAKIA'\n",
    "   739 =  'INVALID: DRONNING MAUD LAND (ANTARCTICA-NORWAY)' \n",
    "   394 =  'INVALID: FRENCH SOUTHERN AND ANTARCTIC'\n",
    "   501 =  'INVALID: GREENLAND'\n",
    "   404 =  'INVALID: GUAM'\n",
    "   730 =  'INVALID: INTERNATIONAL WATERS' \n",
    "   731 =  'INVALID: JOHNSON ISLAND' \n",
    "   471 =  'INVALID: MARIANA ISLANDS, NORTHERN'\n",
    "   737 =  'INVALID: MIDWAY ISLANDS' \n",
    "   753 =  'INVALID: MINOR OUTLYING ISLANDS - USA'\n",
    "   740 =  'INVALID: NEUTRAL ZONE (S. ARABIA/IRAQ)' \n",
    "   710 =  'INVALID: NON-QUOTA IMMIGRANT'\n",
    "   505 =  'INVALID: PUERTO RICO'\n",
    "    0  =  'INVALID: STATELESS'\n",
    "   705 =  'INVALID: STATELESS'\n",
    "   583 =  'INVALID: UNITED STATES'\n",
    "   407 =  'INVALID: UNITED STATES'\n",
    "   999 =  'INVALID: UNKNOWN'\n",
    "   239 =  'INVALID: UNKNOWN COUNTRY'\n",
    "   134 =  'INVALID: USSR'\n",
    "   506 =  'INVALID: U.S. VIRGIN ISLANDS'\n",
    "   755 =  'INVALID: WAKE ISLAND'  \n",
    "   311 =  'Collapsed Tanzania (should not show)'\n",
    "   741 =  'Collapsed Curacao (should not show)'\n",
    "    54 =  'No Country Code (54)'\n",
    "   100 =  'No Country Code (100)'\n",
    "   187 =  'No Country Code (187)'\n",
    "   190 =  'No Country Code (190)'\n",
    "   200 =  'No Country Code (200)'\n",
    "   219 =  'No Country Code (219)'\n",
    "   238 =  'No Country Code (238)'\n",
    "   277 =  'No Country Code (277)'\n",
    "   293 =  'No Country Code (293)'\n",
    "   300 =  'No Country Code (300)'\n",
    "   319 =  'No Country Code (319)'\n",
    "   365 =  'No Country Code (365)'\n",
    "   395 =  'No Country Code (395)'\n",
    "   400 =  'No Country Code (400)'\n",
    "   485 =  'No Country Code (485)'\n",
    "   503 =  'No Country Code (503)'\n",
    "   589 =  'No Country Code (589)'\n",
    "   592 =  'No Country Code (592)'\n",
    "   791 =  'No Country Code (791)'\n",
    "   849 =  'No Country Code (849)'\n",
    "   914 =  'No Country Code (914)'\n",
    "   944 =  'No Country Code (944)'\n",
    "   996 =  'No Country Code (996)'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove quotes\n",
    "country_codes = country_codes.replace(\"'\",'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dictionary to data frame to be able to convert it to parquet file\n",
    "#https://stackoverflow.com/questions/61339594/how-to-convert-a-dictionary-to-dataframe-in-pyspark\n",
    "data_country = {}\n",
    "\n",
    "data_country = split_codes_to_dict(country_codes, \" =  \")\n",
    "df_country_code = spark.createDataFrame(data_country.items(), \n",
    "                      schema=R(fields=[\n",
    "                          Fld(\"country_code\", Str()), \n",
    "                          Fld(\"country_name\", Str())]))\n",
    "\n",
    "df_country_code = df_country_code.withColumn(\"country_code\", df_country_code[\"country_code\"].cast(Int()))\n",
    "df_country_code.show()\n",
    "df_country_code.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete nulls in country_code since it will be the primary key. No null values expected\n",
    "df_country_code = df_country_code.where(col(\"country_code\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#travel code and description and remove quotes\n",
    "travel_code = \"\"\"\n",
    "   1 = 'Air'\n",
    "   2 = 'Sea'\n",
    "   3 = 'Land'\n",
    "   9 = 'Not reported'\"\"\".replace(\"'\",'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dictionary to data frame to be able to convert it to parquet file\n",
    "data_travel = {}\n",
    "\n",
    "data_travel = split_codes_to_dict(travel_code, \" = \")\n",
    "df_travel_code = spark.createDataFrame(data_travel.items(), \n",
    "                      schema=R(fields=[\n",
    "                          Fld(\"travel_code\", Str()), \n",
    "                          Fld(\"travel_name\", Str())]))\n",
    "\n",
    "df_travel_code = df_travel_code.withColumn(\"travel_code\", df_travel_code[\"travel_code\"].cast(Int()))\n",
    "df_travel_code.show()\n",
    "df_travel_code.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete nulls in travel_code since it will be the primary key. No null values expected\n",
    "df_travel_code = df_travel_code.where(col(\"travel_code\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visa_codes = \"\"\"\n",
    "   1 = 'Business'\n",
    "   2 = 'Pleasure'\n",
    "   3 = 'Student'\n",
    "\"\"\".replace(\"'\",'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dictionary to data frame to be able to convert it to parquet file\n",
    "data_visa = {}\n",
    "\n",
    "data_visa = split_codes_to_dict(visa_codes, \" = \")\n",
    "df_visa_code = spark.createDataFrame(data_visa.items(), \n",
    "                      schema=R(fields=[\n",
    "                          Fld(\"visa_code\", Str()), \n",
    "                          Fld(\"visa_name\", Str())]))\n",
    "\n",
    "df_visa_code = df_visa_code.withColumn(\"visa_code\", df_visa_code[\"visa_code\"].cast(Int()))\n",
    "df_visa_code.show()\n",
    "df_visa_code.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete nulls in visa_code since it will be the primary key. No null values expected\n",
    "df_visa_code = df_visa_code.where(col(\"visa_code\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of rows per dataframe to compare to redshift load\n",
    "print((df_immigration.count(), len(df_immigration.columns)))\n",
    "print((df_airport.count(), len(df_airport.columns)))\n",
    "print((df_us_demograhics.count(), len(df_us_demograhics.columns)))\n",
    "print((df_country_code.count(), len(df_country_code.columns)))\n",
    "print((df_travel_code.count(), len(df_travel_code.columns)))\n",
    "print((df_visa_code.count(), len(df_visa_code.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to S3\n",
    "#output_data = \"output/\" #create your own bucket\n",
    "output_data = config.get('S3', 'output_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_demograhics.write.mode('overwrite').parquet(output_data + \"us_demographics/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_data = \"s3a://data-engineering-tourists-to-us-analysis/\"\n",
    "df_immigration.write.mode('overwrite').parquet(output_data + \"us_immigration/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_data = 's3a://data-engineering-tourists-to-us-analysis/'\n",
    "df_airport.write.mode('overwrite').parquet(output_data + \"airport/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_data = \"output/\" #create your own bucket\n",
    "df_country_code.write.mode('overwrite').parquet(output_data + \"country_code/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_data = \"output/\" #create your own bucket\n",
    "df_travel_code.write.mode('overwrite').parquet(output_data + \"travel_code/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_data = \"output/\" #create your own bucket\n",
    "df_visa_code.write.mode('overwrite').parquet(output_data + \"visa_code/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here\n",
    "import psycopg2\n",
    "import configparser\n",
    "from sql_queries import create_table_queries, drop_table_queries, copy_table_queries, insert_table_queries, data_quality_count_check, data_quality_null_references_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_tables(cur, conn):\n",
    "    \"\"\"\n",
    "        Load data in parquet files from S3 to staging tables on Redshift specified in copy_table_queries \n",
    "        \n",
    "        Arguments:\n",
    "            cur - PostgreSQL cursor object\n",
    "            conn - psycopg2 connection instance\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    for query in drop_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tables(cur, conn):\n",
    "    \"\"\"\n",
    "        Load data in parquet files from S3 to staging tables on Redshift specified in copy_table_queries \n",
    "        \n",
    "        Arguments:\n",
    "            cur - PostgreSQL cursor object\n",
    "            conn - psycopg2 connection instance\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    for query in create_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_staging_tables(cur, conn):\n",
    "    \"\"\"\n",
    "        Load data in parquet files from S3 to staging tables on Redshift specified in copy_table_queries \n",
    "        \n",
    "        Arguments:\n",
    "            cur - PostgreSQL cursor object\n",
    "            conn - psycopg2 connection instance\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    for query in copy_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_tables(cur, conn):\n",
    "    \"\"\"\n",
    "        Load data from staging tables to analytics tables specified in insert_table_queries\n",
    "        \n",
    "        Arguments:\n",
    "            cur - PostgreSQL cursor object\n",
    "            conn - psycopg2 connection instance\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    for query in insert_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "#run quality checks to see how many records exist in each redshift table\n",
    "def count_check(cur, conn):\n",
    "    for query in data_quality_count_check:\n",
    "        cur.execute(query)\n",
    "        output = cur.fetchall()\n",
    "        print(\"{} has {} records\".format(query.split(' ')[-1], output[0][0]))\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "#run quality checks to see how many records exist in each redshift table\n",
    "def null_reference_check(cur, conn):\n",
    "    for query in data_quality_null_references_check:\n",
    "        cur.execute(query)\n",
    "        output = cur.fetchall()\n",
    "        print(\"table {}\".format(query.split(' ')[3], output[0][0]))\n",
    "        print(\"column {} has {} null references\".format(query.split(' ')[1], output[0][0]))\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "cur = conn.cursor()\n",
    "\n",
    "drop_tables(cur, conn)\n",
    "create_tables(cur, conn)\n",
    "load_staging_tables(cur, conn)\n",
    "insert_tables(cur, conn)\n",
    "count_check(cur, conn)\n",
    "null_reference_check(cur, conn)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}